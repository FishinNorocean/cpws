Try importing flash-attention for faster inference...
Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:00<00:11,  1.20it/s]Loading checkpoint shards:  13%|█▎        | 2/15 [00:01<00:10,  1.20it/s]Loading checkpoint shards:  20%|██        | 3/15 [00:02<00:09,  1.26it/s]Loading checkpoint shards:  27%|██▋       | 4/15 [00:03<00:08,  1.23it/s]Loading checkpoint shards:  33%|███▎      | 5/15 [00:04<00:07,  1.26it/s]Loading checkpoint shards:  40%|████      | 6/15 [00:04<00:07,  1.26it/s]Loading checkpoint shards:  47%|████▋     | 7/15 [00:05<00:06,  1.31it/s]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:06<00:04,  1.42it/s]Loading checkpoint shards:  60%|██████    | 9/15 [00:06<00:03,  1.52it/s]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:07<00:03,  1.59it/s]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:07<00:02,  1.59it/s]Loading checkpoint shards:  80%|████████  | 12/15 [00:08<00:01,  1.59it/s]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:09<00:01,  1.39it/s]Loading checkpoint shards:  93%|█████████▎| 14/15 [00:10<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 15/15 [00:10<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 15/15 [00:10<00:00,  1.39it/s]
